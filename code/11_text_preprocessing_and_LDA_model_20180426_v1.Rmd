---
title: "Text Pre-Processing, LDA Model, and Posterior Prediction Model"
author: Stephanie Langeland
always_allow_html: yes
output:
  html_document:
    keep_md: true
    toc: true
    number_sections: true
---

```{r, eval = FALSE, echo = FALSE, results = "hide"}
## :::::::::::::::::::: File Info:
## Date: 2018-04-26
## Author: Stephanie Langeland  
## File Name: `11_text_preprocessing_and_LDA_model_20180426_v1`
## Version: 41
## Previous Versions/Files: `11_text_preprocessing_and_LDA_model_20180425_v1`
## Purpose: 
    ## (1) Pre-process FOMC meeting materials.
    ## (2) LDA model for open and close data.
## Input File(s): 
    ## `open_close_data.rds`
    ## `delete_words.csv`
## Dependencies (saved in the "code" folder): 
    ## `LDApriors_rng.stan`
    ## `LDA.stan`
## Output File(s):
    ## `post_20180424.rds` and saved those results to an excel file (post_20180424.xlsx) as well
## Data Output: None
## Required by: Master's Thesis
## Status: Complete
## Machine: 2018 MacBook Pro
## R version: R version 3.4.3 (2017-11-30) -- "Kite-Eating Tree"
```

# Import files & set up the corpus

```{r, warning = FALSE, message = FALSE}
##:::::::::::::::::::::::::::::::::::::::::: Import Data:
library(tm)

path <- "/Users/stephanielangeland/Desktop/Columbia/masters_thesis/Langeland_thesis/input_output_files/open_close_data.rds"

raw_corpus <- readRDS(path)

library(dplyr)
```

Extract statements:

```{r}
stmt_sub <- raw_corpus %>%
  filter(!is.na(stmt_text))
```

Extract minutes:

```{r}
min_sub <- raw_corpus %>%
  filter(!is.na(min_text))
```

Stack minutes and statements, with their corresponding meta data (response 
variables), into a data frame:

```{r}
texts_orig <- data.frame(
  doc_id = c(paste0(stmt_sub$release_date, "_s"),
             paste0(min_sub$release_date, "_m")),
  text = c(stmt_sub$stmt_text,
           min_sub$min_text),
  release_date = c(stmt_sub$release_date,
                   min_sub$release_date),
  stringsAsFactors = F)
```

Merge the statements and minutes text with the meta data (response variables):

```{r}
texts <- merge(
  x = texts_orig,
  y = raw_corpus[ , c(2, ## release date
                      5:ncol(raw_corpus))], ## response variables
  by = "release_date"
  )

colnames(texts)

#str(texts)

#View(texts)
```

Combine documents that were released on the same day:

```{r}
##:::::::::::::::::::: On which dates were multiple docs released on same day:
dup_dates_location <- which(duplicated(texts$release_date) == TRUE)

duplicated_date <- texts[dup_dates_location, "release_date"] 
duplicated_date ## only one date when multiple (two) docs were released

dup_dates2 <- which(texts$release_date == duplicated_date) 
dup_dates2 ## rows 44 and 45 are the same date



##:::::::::::::::::::: Combine documents that were released on the same 
##:::::::::::::::::::: day - combine rows 44 and 45 into row 44:
texts$text[ dup_dates2[1] ] <- paste( 
  texts[ dup_dates2[1], c("text")], ## row 44
  texts[ dup_dates2[2], c("text")] ## row 45
                                      )



##:::::::::::::::::::: Update the doc_id in row 44:
texts$doc_id[ dup_dates2[1] ] <- paste(
  duplicated_date, 
  "_ms",
  sep = ""
)



##:::::::::::::::::::: Delete the duplicate row (row 45):
texts <- texts[-dup_dates2[2], ]



###::::::::::::::::::::  Verify that there are no more duplicate dates:
summary(duplicated(texts$release_date))
```

Use the `stringr` package to do this now because the `removePunctuation` function isn't 
catching everything later on and remove this special character:

```{r, warning = FALSE, message = FALSE}
library(stringr)

texts$text <- str_replace_all(
  texts$text,
  "[:punct:]",
  " "
) 

texts$text <- str_replace_all(
  texts$text,
  "ï¿½",
  ""
) 
```

Fix spelling errors:

```{r}
texts$text <- str_replace_all(
  texts$text,
  "accomodative",
  "accommodative"
) 
```

No real difference between "economy" vs. "economic" for a machine:

```{r}
## use "economic" instead because it is said more times than "economy":
text_sub <- unlist(
  texts$text
)

text_sub <- tolower(text_sub)

sum(
  str_count(text_sub,
            "economy")
)

sum(
  str_count(text_sub,
            "economic")
)

texts$text <- str_replace_all(
  texts$text,
  "economy",
  "economic"
) 
```

No real difference between "accrue" vs. "accrual" for a machine:

```{r}
texts$text <- str_replace_all(
  texts$text,
  "accrued",
  "accrue"
) 

texts$text <- str_replace_all(
  texts$text,
  "accruing",
  "accrue"
) 

texts$text <- str_replace_all(
  texts$text,
  "accrual",
  "accrue"
) 

texts$text <- str_replace_all(
  texts$text,
  "accruals",
  "accrue"
) 
```

Create a corpus data frame:

```{r}
df_source <- DataframeSource(texts) ## this is the source of the corpus data frame

df_corpus <- VCorpus(df_source) ## create a volatile corpus

#View(df_corpus)

print(df_corpus) ## contains the correct number of documents

#inspect(df_corpus)

head(meta(df_corpus)) ## response variables are saved as meta data
```

# Pre-process the corpus `df_corpus`

Convert all words to lowercase:

```{r}
df_corpus <- tm_map(
  df_corpus, 
  content_transformer(tolower)
)
```

Remove white space:

```{r}
df_corpus <- tm_map(
  df_corpus, 
  stripWhitespace
) 
```

Remove numbers:

```{r}
df_corpus <- tm_map(
  df_corpus, 
  removeNumbers
)
```

Remove stop words:

```{r}
df_corpus <- tm_map(
  df_corpus, 
  removeWords, 
  stopwords("english")
)
```

Remove Fed employees' names and titles, FRB cities, months, days of the week,
country names and abbreviations, punctuation; stem words to the root:

```{r}
delete_words <- read.csv(
  "/Users/stephanielangeland/Desktop/Columbia/masters_thesis/Langeland_thesis/input_output_files/delete_words.csv"
)

str(delete_words)

delete_words$delete_words <- as.character(delete_words$delete_words)

delete_words$delete_words <- tolower(delete_words$delete_words) ## convert to lower case

delete_words <- unlist(delete_words$delete_words)

df_corpus <- tm_map(
  df_corpus, 
  removeWords, 
  delete_words
)



##:::::::::::::::::::: Delete names of U.S. States: 
df_corpus <- tm_map(
  df_corpus, 
  removeWords, 
  tolower(state.abb) ## state abbreviations
)

df_corpus <- tm_map(
  df_corpus, 
  removeWords, 
  tolower(state.name) ## full names of states
)

df_corpus <- tm_map(
  df_corpus, 
  removeWords, 
  tolower(state.region) ## U.S. regions
)



##:::::::::::::::::::: Remove punctuation:
df_corpus <- tm_map(
  df_corpus, 
  removePunctuation
)
```

```{r, message = FALSE, warning = FALSE}
##:::::::::::::::::::: Delete names of U.S. cities: 
library(maps)

cities <- unlist(
  tolower(
    us.cities$name
  )
)



##:::::::::::::::::::: Stem words:
df_corpus <- tm_map(
  df_corpus, 
  stemDocument
)
``` 

# Create a document-term matrix (`dtm`)

```{r}
dtm <- DocumentTermMatrix(
  df_corpus
  )

#dim(dtm)
str(dtm)
dtm

dtm_mat <- as.matrix(dtm)
```

```{r}
set.seed(1234)

dtm1 <- removeSparseTerms(dtm,
                          sparse = 0.3) 

dtm1

dtm1_mat <- as.matrix(dtm1)

head(dtm1_mat)

#View(sort(colnames(dtm1_mat)))
```

# Model inputs

```{r}
##:::::::::::::::::::: Number of documents:
M <- nrow(dtm1_mat) 



##:::::::::::::::::::: Integer array of size M - each of its elements is the 
##:::::::::::::::::::: number of words in that document:
N <- rowSums(dtm1_mat) 



##:::::::::::::::::::: Number of words in the vocabulary which is the total
##:::::::::::::::::::: number of words: 
V <- ncol(dtm1_mat) 
```

Some experimentation analysis led me to choose the following priors/model 
inputs:

```{r}
##:::::::::::::::::::: Number of topics:
K <- 3

##:::::::::::::::::::: Beta:
beta_3 <- rep(
  200 / V, 
  V ## vector size V
  )



##:::::::::::::::::::: Alpha:
##::::::::: I experimented with alpha: lower values yield more 
##::::::::: polarized outcomes and higher yield more concentrated outcomes
##::::::::: so am aiming for higher values):
alpha_3 <- rep(
  100 / K,
  K ## size K
  )
```

# Prior predicitve distribution

```{r, message = FALSE, warning = FALSE}
library(rstan)
library(rstanarm)
```

```{r, message = FALSE, warning = FALSE}
setwd("/Users/stephanielangeland/Desktop/Columbia/masters_thesis/Langeland_thesis/code")

expose_stan_functions("LDApriors_rng.stan")

prior_LDA_dtm_1 <- LDA_rng(
  V = V,
  beta = beta_3,
  K = K,
  alpha = alpha_3,
  M = M, 
  N = N
)
```

```{r, eval = FALSE}
##:::::::::::::::::::: How closely does my prior document term matrix
##:::::::::::::::::::: (`prior_LDA_dtm_1`) resemble my actual document term
##:::::::::::::::::::: matrix (`dtm1_mat`):

#View(prior_LDA_dtm_1)
#View(dtm1_mat)

#summary(prior_LDA_dtm_1 - dtm1_mat) == 0
hist(prior_LDA_dtm_1 - dtm1_mat)

summary(
  rowSums(prior_LDA_dtm_1) - 
    rowSums(dtm1_mat)
)

#sum((prior_LDA_dtm_1 - dtm1_mat) == 0)
#sum((prior_LDA_dtm_1 - dtm1_mat) != 0)

round(
  (
  sum( ( prior_LDA_dtm_1 - dtm1_mat) == 0 ) / 
    ( ncol(dtm1_mat) * nrow(dtm1_mat) )
       * 100 
       ), 
      digits = 2
  ) ## percentage of `prior_LDA_dtm_1` values that match `dtm1_mat`
```

# LDA model and posterior prediction model of past data

```{r, message = FALSE, warning = FALSE}
library(tidytext)

library(dplyr)
```

```{r}
##:::::::::::::::::::: Convert `df_corpus` into a single data frame column:
text_df <- as.data.frame(
  do.call(
    rbind, 
    lapply(
      df_corpus, 
      FUN = as.character
      )
  ), 
  stringsAsFactors = FALSE
) 



##:::::::::::::::::::: Rename the column:
colnames(text_df) <- "text"



##:::::::::::::::::::: `doc_id`:
text_df$doc <- 1:nrow(text_df)



##:::::::::::::::::::: Get word stems:
text_df <- text_df %>% 
  unnest_tokens(
    wordstem, 
    text
  )



##:::::::::::::::::::: Create a flattened dtm:
text_df <- text_df[text_df$wordstem %in% 
                     colnames(dtm1_mat), ]



##:::::::::::::::::::: Create a data frame of the response variables
##:::::::::::::::::::: (`close_perc_open` variables only):
response_vars <- meta(df_corpus)

response_vars_all <- response_vars[ , 49:54] 

summary(is.na(response_vars_all)) 
##^ `CLAComdty_close_perc_open` & `USSOC_close_perc_open` suffer from missingness
## which cannot be accepted by the below posterior prediction stan model so must 
## eliminate those variables.



##:::::::::::::::::::: Final response variables data frame (without 
##:::::::::::::::::::: `CLAComdty_close_perc_open` & `USSOC_close_perc_open`):
response_vars_sub <- response_vars_all[ , c(2:4, 6)] 



##:::::::::::::::::::: Input data for the below posterior prediction stan model:
dat <- with(text_df, 
            list(
              
              K = K, 
              
              V = ncol(dtm1_mat), 
              
              M = nrow(dtm1_mat), 
              
              N = nrow(text_df),
              
              w = as.integer(factor(
                wordstem, 
                levels = colnames(dtm1_mat)
                )
                             ),
              
              doc = doc, 
              
              alpha_mean = 1, 
              
              beta = rep( 1 / K, 
                          ncol(dtm1_mat)
                          ),
              
              P = ncol(response_vars_sub), ## number of stocks
              
              Y = response_vars_sub,  ## price changes 
              
              alpha_stock = 0, ## intercept: probability that this stock prices swings this much per day
              
              beta_stock_raw =  0, 
              
              tau_mean = 1, ## standard deviation in coefficient across words
              
              sigma_mean = 6, ## std devs on non-fomc and fomc dates for spx closing prices
              
              DTM = ( dtm1_mat / rowSums(dtm1_mat) )
              
              )
            )
```

```{r, eval = FALSE}
##:::::::::::::::::::: Posterior prediction stan model:
start_time <- Sys.time() ## record how long the model takes to run


post <- stan("LDA.stan", 
             data = dat, 
             cores = parallel::detectCores()
             ) 


end_time <- Sys.time() ## record how long the model takes to run

end_time - start_time ## how long the stan model took to run (about 8.5 hours on 4 cores)


setwd("/Users/stephanielangeland/Desktop/Columbia/masters_thesis/Langeland_thesis/input_output_files")

saveRDS(post,
        file = "post_20180424.rds") ## save the output of the stan mdoel
```

```{r}
test <- readRDS(
  "/Users/stephanielangeland/Desktop/Columbia/masters_thesis/Langeland_thesis/input_output_files/post_20180424.rds"
  )

post <- test

#post
```

## Explaination of the output (`post`)

_Refer to the paper for more details._

1) The chains appear to be going to different places because the labels of the
categories can be swapped without changing the probability of observing the
data, even though specifying `alpha` to be a positive ordered vector was supposed
to prevent that. But each of those imply the same distribution for everything
that is not `theta` or `alpha`. And within each chain, the Rhat values for
`theta` and `alpha` are acceptable.  Furthermore, the `Y_tilde` Rhat values
are all 1, which suggests that the chains converged.

2) The above output represents the results of the LDA model that fed a posterior
prediction model of past data.  This model first used LDA on the words of the
FOMC post-meeting announcements and minutes.  Then a prediction model was applied
to predict changes in securities on the days of Fed releases.  The above output
shows the posterior predictions of changes in the response variables, which is
represented within `Y_tilde`.  The response variables are the 10-year constant
maturity rate (`H15T10Y`), Chicago Board Options Exchange (CBOE) interest rate
on the 10 year Treasury Note (`TNX`), S&P 500 Index (`SPX`), and CBOE VIX Index
(`VIX`).  These variables were recorded as the closing value as a percentage of
the opening value.  Therefore, in the raw data (`response_vars_sub`), values
greater than 1 mean that the closing value was greater than the opening so the
security's value went up after the Fed released materials.  Inversely, values
less than 1 mean that the closing value was less than the opening so the security's
value went down after the Fed released materials.  To ensure that an increase (to
simplify the analysis of the output) is always "an improvement to market conditions",
the reciprocal of the transformed (close / open) VIX is recorded in the raw data
(`response_vars_sub`) because declines in the VIX signal subdued volatility but
I wanted all variables to be consistent in that an increase is always a "good"
market reaction. Please refer to the below section for an analysis of the
differences between the output (`Y_tilde`) and the actual response variables
(`response_vars_sub`).

```{r}
Y_tilde <- extract(post, 
                   pars = "Y_tilde", 
                   permuted = FALSE
                   )

#Y_tilde

length(Y_tilde)

dim(Y_tilde)
```

## Explaination of the output (`Y_tilde`)

_Refer to the paper for more details._

`Y_tilde` is the number of iterations (`r {dim(Y_tilde)[1]}`) by the number of
chains (`r {dim(Y_tilde)[2]}`) by the number of Fed days (352) times the number of
response variables (4) (which equals `r {dim(Y_tilde)[3]}`).  Each `Y_tilde` matrix
was collapsed into a vector.  There is 1 matrix for each iteration and chain
within `Y_tilde`.

# Posterior predictive checks

_Refer to the paper for more details._

## Check how well the model predicts the response variables

Create a matrix that contains the differences between the predictions and actual
response variables, then calculate the average of all of those posterior draws:

```{r}
options(scipen = 999)

model_residuals <- apply(
  Y_tilde, 
  1:2, 
  FUN = function(y) response_vars_sub - matrix(y, 
                                               nrow = 352, 
                                               ncol = 4)
)


model_residuals_avg <- Reduce("+", model_residuals) / length(model_residuals)

str(model_residuals_avg)

head(model_residuals_avg)

model_residuals_avg_abs <- abs(model_residuals_avg) ## take the absolute value
```

### Range rediction accuracy

_Check to see how close the predictions are to the actual response variables_
_(within a range - one standard deviation of the corresponding response variable)._

__`H15T10Y_close_perc_open`	- 10-year constant maturity rate:__

```{r}
##:::::::::::::::::::: Are the predictions within 1 standard deviation of
##:::::::::::::::::::: `H15T10Y_close_perc_open`?:
model_residuals_avg_abs_rng_h15 <- ifelse(
  model_residuals_avg_abs$H15T10Y_close_perc_open <= sd(response_vars_sub$H15T10Y_close_perc_open),
  "within_range",
  "out_of_range"
)

model_residuals_avg_abs_rng_h15 <- as.data.frame(model_residuals_avg_abs_rng_h15)

summary(model_residuals_avg_abs_rng_h15)


##::::::::: Percentage of each variable that was predicted "within range":
rng_h15_pred <- sum(model_residuals_avg_abs_rng_h15 == "within_range") / 
  nrow(model_residuals_avg_abs_rng_h15) * 100

rng_h15_pred
```

`r {round(rng_h15_pred, digits = 2)}`% of the time, the model correctly predicted
the change in the 10-year constant maturity rate within 1 standard deviation of
observed data for the 10-year constant maturity rate.

__`SPX_close_perc_open` - S&P 500 Index:__

```{r}
##:::::::::::::::::::: Are the predictions within 1 standard deviation of
##:::::::::::::::::::: `SPX_close_perc_open`?:
model_residuals_avg_abs_rng_spx <- ifelse(
  model_residuals_avg_abs$SPX_close_perc_open <= sd(response_vars_sub$SPX_close_perc_open),
  "within_range",
  "out_of_range"
)

model_residuals_avg_abs_rng_spx <- as.data.frame(model_residuals_avg_abs_rng_spx)

summary(model_residuals_avg_abs_rng_spx)


##::::::::: Percentage of each variable that was predicted "within range":
rng_spx_pred <- sum(model_residuals_avg_abs_rng_spx == "within_range") / 
  nrow(model_residuals_avg_abs_rng_spx) * 100

rng_spx_pred
```

`r {round(rng_spx_pred, digits = 2)}`% of the time, the model correctly predicted
the change in the S&P 500 Index within 1 standard deviation of observed data for
the S&P 500 Index.

__`TNX_close_perc_open` - CBOE interest rate on the 10 year Treasury Note:__

```{r}
##:::::::::::::::::::: Are the predictions within 1 standard deviation of
##:::::::::::::::::::: `TNX_close_perc_open`?:
model_residuals_avg_abs_rng_tnx <- ifelse(
  model_residuals_avg_abs$TNX_close_perc_open <= sd(response_vars_sub$TNX_close_perc_open),
  "within_range",
  "out_of_range"
)

model_residuals_avg_abs_rng_tnx <- as.data.frame(model_residuals_avg_abs_rng_tnx)

summary(model_residuals_avg_abs_rng_tnx)


##::::::::: Percentage of each variable that was predicted "within range":
rng_tnx_pred <- sum(model_residuals_avg_abs_rng_tnx == "within_range") / 
  nrow(model_residuals_avg_abs_rng_tnx) * 100

rng_tnx_pred
```

`r {round(rng_tnx_pred, digits = 2)}`% of the time, the model correctly predicted
the change in the CBOE interest rate on the 10 year Treasury Note within 1
standard deviation of observed data for this rate.

__`VIX_close_perc_open`	- VIX Index:__

```{r}
##:::::::::::::::::::: Are the predictions within 1 standard deviation of
##:::::::::::::::::::: `VIX_close_perc_open`?:
model_residuals_avg_abs_rng_vix <- ifelse(
  model_residuals_avg_abs$VIX_close_perc_open <= sd(response_vars_sub$VIX_close_perc_open),
  "within_range",
  "out_of_range"
)

model_residuals_avg_abs_rng_vix <- as.data.frame(model_residuals_avg_abs_rng_vix)

summary(model_residuals_avg_abs_rng_vix)


##::::::::: Percentage of each variable that was predicted "within range":
rng_vix_pred <- sum(model_residuals_avg_abs_rng_vix == "within_range") / 
  nrow(model_residuals_avg_abs_rng_vix) * 100

rng_vix_pred
```

`r {round(rng_vix_pred, digits = 2)}`% of the time, the model correctly predicted
the change in the VIX within 1 standard deviation of observed data for the VIX.

Overall, the model correctly predicted the changes in the closing value as a
percentage of the opening value, within 1 standard deviation of each respective
response variable, well.

### Directional prediction accuracy

It is also a problem if the predictions are going in the wrong  direction e.g.,
if any of the `response_vars_sub` values > 1 then the close > open price so the
value went up after the Fed release. On the other hand, if the
`response_vars_sub` < 1 then the close < open price so the value went down after
the Fed release.  If the prediction > 1 and the corresponding `response_vars_sub`
value < 1 then the model predicted the security moving in the wrong direction
which is an issue.  In this section , I create a measure of whether
`model_residuals_avg` accurately predicts the direction of the response variables
(`response_vars_sub`).

Put the predictions in matrix format for each chain and iteration:

```{r}
##:::::::::::::::::::: Convert the post output to a matrix:
post_mat <- as.matrix(post) 

str(post_mat)



##:::::::::::::::::::: Extract the `Y_tilde` predictions from `post_mat`:
post_mat_Y_tilde <- post_mat[ , 
                              grep("Y_tilde", 
                                   colnames(post_mat), 
                                   fixed = TRUE)
                              ]

str(post_mat_Y_tilde)

dim(post_mat_Y_tilde)
```

_Note:_ The column names in `post_mat_Y_tilde` correspond to the variables in 
`response_vars_sub` so:

  * Any column name with a ",1]" corresponds to "`r {colnames(response_vars_sub)[1]}`"
  (`colnames(response_vars_sub)[1]`).
  
  * Any column name with a ",2]" corresponds to "`r {colnames(response_vars_sub)[2]}`"
  (`colnames(response_vars_sub)[2]`).
  
   * Any column name with a ",3]" corresponds to "`r {colnames(response_vars_sub)[3]}`"
  (`colnames(response_vars_sub)[3]`).
  
   * Any column name with a ",4]" corresponds to "`r {colnames(response_vars_sub)[4]}`"
  (`colnames(response_vars_sub)[4]`).

__Overall:__

```{r}
overall_direction <- table(
  observed = sign(as.matrix(response_vars_sub) - 1), 
  predicted = sign(colMeans(post_mat_Y_tilde) - 1)
  )

rownames(overall_direction)[1] <- "closing < opening"
colnames(overall_direction)[1] <- "closing < opening"

rownames(overall_direction)[2] <- "closing = opening (no change)"

rownames(overall_direction)[3] <- "closing > opening"
colnames(overall_direction)[2] <- "closing > opening"

overall_direction
```

Overall, for all variables:

1) The model best predicts `closing > opening`

2) It does a bad job of predicting `closing < opening`

3) It NEVER predicts `closing = opening`

__`H15T10Y_close_perc_open`	- 10-year constant maturity rate:__

```{r}
H15T10Y_direction <- table(
  observed = sign(as.matrix(response_vars_sub$H15T10Y_close_perc_open) - 1), 
  predicted = sign(colMeans(post_mat_Y_tilde[ ,
                           grep(",1]", ## because colnames(response_vars_sub)[1]
                                colnames(post_mat_Y_tilde), 
                                fixed = TRUE)
                           ]) - 1)
  )

rownames(H15T10Y_direction)[1] <- "closing < opening"
colnames(H15T10Y_direction)[1] <- "closing < opening"

rownames(H15T10Y_direction)[2] <- "closing = opening (no change)"

rownames(H15T10Y_direction)[3] <- "closing > opening"
colnames(H15T10Y_direction)[2] <- "closing > opening"


H15T10Y_direction
```

For the 10-year constant maturity rate: bad predictions overall.

__`SPX_close_perc_open` - S&P 500 Index:__

```{r}
SPX_direction <- table(
  observed = sign(as.matrix(response_vars_sub$SPX_close_perc_open) - 1), 
  predicted = sign(colMeans(post_mat_Y_tilde[ ,
                           grep(",2]", ## because colnames(response_vars_sub)[2]
                                colnames(post_mat_Y_tilde), 
                                fixed = TRUE)
                           ]) - 1)
  )

rownames(SPX_direction)[1] <- "closing < opening"
colnames(SPX_direction)[1] <- "closing < opening"

rownames(SPX_direction)[2] <- "closing > opening"
colnames(SPX_direction)[2] <- "closing > opening"

SPX_direction
```

For the S&P 500 Index:

1) The model best predicts `closing > opening`

2) It does a horrible job of predicting `closing < opening`

__`TNX_close_perc_open` - CBOE interest rate on the 10 year Treasury Note:__

```{r}
TNX_direction <- table(
  observed = sign(as.matrix(response_vars_sub$TNX_close_perc_open) - 1), 
  predicted = sign(colMeans(post_mat_Y_tilde[ ,
                           grep(",3]", ## because colnames(response_vars_sub)[3]
                                colnames(post_mat_Y_tilde), 
                                fixed = TRUE)
                           ]) - 1)
  )

rownames(TNX_direction)[1] <- "closing < opening"
colnames(TNX_direction)[1] <- "closing < opening"

rownames(TNX_direction)[2] <- "closing = opening (no change)"

rownames(TNX_direction)[3] <- "closing > opening"
colnames(TNX_direction)[2] <- "closing > opening"

TNX_direction
```

For the CBOE interest rate on the 10 year Treasury Note:

1) The model does an okay job of predicting `closing > opening`

2) It does a horrible job of predicting `closing < opening`

3) It NEVER predicts `closing = opening`

__`VIX_close_perc_open`	- VIX Index:__

```{r}
VIX_direction <- table(
  observed = sign(as.matrix(response_vars_sub$VIX_close_perc_open) - 1), 
  predicted = sign(colMeans(post_mat_Y_tilde[ ,
                           grep(",4]", ## because colnames(response_vars_sub)[4]
                                colnames(post_mat_Y_tilde), 
                                fixed = TRUE)
                           ]) - 1)
  )

rownames(VIX_direction)[1] <- "closing < opening"
colnames(VIX_direction)[1] <- "closing < opening"

rownames(VIX_direction)[2] <- "closing = opening (no change)"

rownames(VIX_direction)[3] <- "closing > opening"
colnames(VIX_direction)[2] <- "closing > opening"

VIX_direction
```

For the VIX:

1) The model does a good job of predicting `closing > opening`

2) It does a horrible job of predicting `closing < opening`

3) It NEVER predicts `closing = opening`

## Model fit 

```{r, message = FALSE, warning = FALSE}
library(ggplot2)
library(bayesplot)
```

### Density overlay plots

Compare the empirical distribution of the data _Y_ to the distributions of
simulated/replicated data _Yrep_ from the posterior predictive distribution.
How well do predictions from the posterior distribution match the true outcome
variable that was used to fit the data?

__`H15T10Y_close_perc_open`	- 10-year constant maturity rate:__

```{r}
pp_check(
  object = response_vars_sub$H15T10Y_close_perc_open, 
  yrep = post_mat_Y_tilde[ ,
                           grep(",1]", ## because colnames(response_vars_sub)[1]
                                colnames(post_mat_Y_tilde), 
                                fixed = TRUE)
                           ],
  fun = ppc_dens_overlay
  ) + 
  ggtitle("10Y Constant Maturity Rate (Y) versus Predictions (Yrep)")
```

It appears that observed data are more concentrated than the predictions.  Thus,
the model produced more extreme predictions than what actually occurred in the data. 

__`SPX_close_perc_open` - S&P 500 Index:__

```{r}
pp_check(
  object = response_vars_sub$SPX_close_perc_open, 
  yrep = post_mat_Y_tilde[ ,
                           grep(",2]", ## because colnames(response_vars_sub)[2]
                                colnames(post_mat_Y_tilde), 
                                fixed = TRUE)
                           ],
  fun = ppc_dens_overlay
  ) + 
  ggtitle("S&P 500 Index (Y) versus Predictions (Yrep)")
```

It appears that observed data are more concentrated than the predictions.  Thus,
the model produced more extreme predictions than what actually occurred in the data. 

__`TNX_close_perc_open` - CBOE interest rate on the 10 year Treasury Note:__

```{r}
pp_check(
  object = response_vars_sub$TNX_close_perc_open, 
  yrep = post_mat_Y_tilde[ ,
                           grep(",3]", ## because colnames(response_vars_sub)[3]
                                colnames(post_mat_Y_tilde), 
                                fixed = TRUE)
                           ],
  fun = ppc_dens_overlay
  ) + 
  ggtitle("10Y Treasury Note CBOE Interest Rate (Y)\nversus Predictions (Yrep)")
```

It appears that observed data are more concentrated than the predictions.  Thus,
the model produced more extreme predictions than what actually occurred in the data. 

__`VIX_close_perc_open`	- VIX Index:__

```{r}
pp_check(
  object = response_vars_sub$VIX_close_perc_open, 
  yrep = post_mat_Y_tilde[ ,
                           grep(",4]", ## because colnames(response_vars_sub)[4]
                                colnames(post_mat_Y_tilde), 
                                fixed = TRUE)
                           ],
  fun = ppc_dens_overlay
  ) + 
  ggtitle("VIX Index (Y) versus Predictions (Yrep)")
```

It appears that predictions are more concentrated than the observed data.  Thus,
the model produced less extreme predictions than what actually occurred in the data. 

### Marginal distribution plots

Compare the proportion of the observed data that fall within 50% predictive
intervals.

__`H15T10Y_close_perc_open`	- 10-year constant maturity rate:__

```{r}
##:::::::::::::::::::: Numerical Assessment of Calibration:
lower_h15 <- apply(
  post_mat_Y_tilde[ ,
                                       grep(",1]", ## because colnames(response_vars_sub)[1]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
  MARGIN = 2, 
  quantile, 
  probs = 0.25
)


upper_h15 <- apply(
  post_mat_Y_tilde[ ,
                                       grep(",1]", ## because colnames(response_vars_sub)[1]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
  MARGIN = 2, 
  quantile, 
  probs = 0.75
)


h15_fit <- mean(response_vars_sub$H15T10Y_close_perc_open > lower_h15 & 
       response_vars_sub$H15T10Y_close_perc_open < upper_h15)

h15_fit



##:::::::::::::::::::: Graphical Assessment of Calibration:
ppc_intervals(y = response_vars_sub$H15T10Y_close_perc_open, 
              yrep = post_mat_Y_tilde[ ,
                                       grep(",1]", ## because colnames(response_vars_sub)[1]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
              prob = 0.5
) +
  ggtitle(
    paste(
      "Posterior Predictive Interval with Probability of 0.5 for 10Y Constant\nMaturity Rate (Y) versus Predictions (Yrep)",
      "\n(Numerical Assessment of Fit = ",
      print(
        round( h15_fit * 100, digits = 2)
      ),
      "%)"
    )
  )
```

Since the `h15_fit` > 50%, the model is over fitting because
`r {round( (h15_fit * 100), digits = 2)}`% of the data are fitting in the 50%
interval.

__`SPX_close_perc_open` - S&P 500 Index:__

```{r}
##:::::::::::::::::::: Numerical Assessment of Calibration:
lower_spx <- apply(
  post_mat_Y_tilde[ ,
                                       grep(",2]", ## because colnames(response_vars_sub)[2]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
  MARGIN = 2, 
  quantile, 
  probs = 0.25
)


upper_spx <- apply(
  post_mat_Y_tilde[ ,
                                       grep(",2]", ## because colnames(response_vars_sub)[2]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
  MARGIN = 2, 
  quantile, 
  probs = 0.75
)


spx_fit <- mean(response_vars_sub$SPX_close_perc_open > lower_spx & 
       response_vars_sub$SPX_close_perc_open < upper_spx)

spx_fit



##:::::::::::::::::::: Graphical Assessment of Calibration:
ppc_intervals(y = response_vars_sub$SPX_close_perc_open, 
              yrep = post_mat_Y_tilde[ ,
                                       grep(",2]", ## because colnames(response_vars_sub)[2]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
              prob = 0.5
) +
  ggtitle(
    paste(
      "Posterior Predictive Interval with Probability of 0.5 for\nS&P 500 Index (Y) versus Predictions (Yrep)",
      "\n(Numerical Assessment of Fit = ",
      print(
        round( spx_fit * 100, digits = 2)
      ),
      "%)"
    )
  )
```

Since the `spx_fit` > 50%, the model is over fitting because
`r {round( (spx_fit * 100), digits = 2)}`% of the data are fitting in the 50%
interval.

__`TNX_close_perc_open` - CBOE interest rate on the 10 year Treasury Note:__

```{r}
##:::::::::::::::::::: Numerical Assessment of Calibration:
lower_tnx <- apply(
  post_mat_Y_tilde[ ,
                                       grep(",3]", ## because colnames(response_vars_sub)[3]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
  MARGIN = 2, 
  quantile, 
  probs = 0.25
)


upper_tnx <- apply(
  post_mat_Y_tilde[ ,
                                       grep(",3]", ## because colnames(response_vars_sub)[3]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
  MARGIN = 2, 
  quantile, 
  probs = 0.75
)


tnx_fit <- mean(response_vars_sub$TNX_close_perc_open > lower_tnx & 
       response_vars_sub$TNX_close_perc_open < upper_tnx)

tnx_fit



##:::::::::::::::::::: Graphical Assessment of Calibration:
ppc_intervals(y = response_vars_sub$TNX_close_perc_open, 
              yrep = post_mat_Y_tilde[ ,
                                       grep(",3]", ## because colnames(response_vars_sub)[3]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
              prob = 0.5
) +
  ggtitle(
    paste(
      "Posterior Predictive Interval with Probability of 0.5\nfor 10Y Treasury Note CBOE Interest Rate (Y)\nversus Predictions (Yrep)",
      "\n(Numerical Assessment of Fit = ",
      print(
        round( tnx_fit * 100, digits = 2)
      ),
      "%)"
    )
  )
```

Since the `tnx_fit` > 50%, the model is over fitting because
`r {round( (tnx_fit * 100), digits = 2)}`% of the data are fitting in the 50%
interval.

__`VIX_close_perc_open`	- VIX Index:__

```{r}
##:::::::::::::::::::: Numerical Assessment of Calibration:
lower_vix <- apply(
  post_mat_Y_tilde[ ,
                                       grep(",4]", ## because colnames(response_vars_sub)[4]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
  MARGIN = 2, 
  quantile, 
  probs = 0.25
)


upper_vix <- apply(
  post_mat_Y_tilde[ ,
                                       grep(",4]", ## because colnames(response_vars_sub)[4]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
  MARGIN = 2, 
  quantile, 
  probs = 0.75
)


vix_fit <- mean(response_vars_sub$VIX_close_perc_open > lower_vix & 
       response_vars_sub$VIX_close_perc_open < upper_vix)

vix_fit



##:::::::::::::::::::: Graphical Assessment of Calibration:
ppc_intervals(y = response_vars_sub$VIX_close_perc_open, 
              yrep = post_mat_Y_tilde[ ,
                                       grep(",4]", ## because colnames(response_vars_sub)[4]
                                            colnames(post_mat_Y_tilde), 
                                            fixed = TRUE)
                                       ], 
              prob = 0.5
) +
  ggtitle(
    paste(
      "Posterior Predictive Interval with Probability of 0.5\nfor VIX Index (Y) versus Predictions (Yrep)",
      "\n(Numerical Assessment of Fit = ",
      print(
        round( vix_fit * 100, digits = 2)
      ),
      "%)"
    )
  )
```

Since the `vix_fit` < 50%, the model is under fitting because
`r {round( (vix_fit * 100), digits = 2)}`% of the data are fitting in the 50%
interval.

```{r, eval = FALSE, echo = FALSE}
##:::::::::::::::::::: Save workspace
save.image(file = "/Users/stephanielangeland/Desktop/Columbia/masters_thesis/Langeland_thesis/code/11_text_preprocessing_and_LDA_model_20180426_v1.RData")

savehistory(file = "/Users/stephanielangeland/Desktop/Columbia/masters_thesis/Langeland_thesis/code/history_11_text_preprocessing_and_LDA_model_20180426_v1.RData")
```
